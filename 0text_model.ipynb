{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b450f5b-41a0-4989-b0af-fde553fc4c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 15:28:44.684960: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-25 15:28:44.692570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753475324.700950   49008 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753475324.703644   49008 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753475324.710339   49008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753475324.710348   49008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753475324.710349   49008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753475324.710349   49008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-25 15:28:44.712558: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/yeblad/anaconda3/envs/capstone_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Symptom Classifier using DistilBERT\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers, models, callbacks, Model\n",
    "from tensorflow.keras.applications import EfficientNetV2B0, DenseNet121\n",
    "import cv2\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f0dc55-439e-421e-9b4c-1f37ca0f741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (250, 2)\n",
      "Columns: ['text', 'label']\n",
      "Sample data:\n",
      "                                                text        label\n",
      "0  Feeling like cough and nasal congestion for a ...  Urgent Care\n",
      "1  My child has low-grade fever and headache and ...  Urgent Care\n",
      "2  Noticed painful urination and I'm not sure wha...  Urgent Care\n",
      "3  Noticed swollen ankle, no deformity since yest...  Urgent Care\n",
      "4           Noticed scalp tenderness since yesterday  Urgent Care\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "Urgent Care    125\n",
      "ER             125\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv(\"urgentcare_symptoms_dataset.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Sample data:\")\n",
    "print(df.head())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ae01aa-3601-4603-9b99-2dd7a0b41d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a97f9c-4cb8-44ec-9053-b3b07d83f9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping:\n",
      "  ER -> 0\n",
      "  Urgent Care -> 1\n",
      "\n",
      "Label ID range: 0 to 1\n",
      "Sample with encoded labels:\n",
      "                                                text        label  label_id\n",
      "0  Feeling like cough and nasal congestion for a ...  Urgent Care         1\n",
      "1  My child has low-grade fever and headache and ...  Urgent Care         1\n",
      "2  Noticed painful urination and I'm not sure wha...  Urgent Care         1\n",
      "3  Noticed swollen ankle, no deformity since yest...  Urgent Care         1\n",
      "4           Noticed scalp tenderness since yesterday  Urgent Care         1\n"
     ]
    }
   ],
   "source": [
    "unique_labels = sorted(df['label'].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"  {label} -> {idx}\")\n",
    "\n",
    "# Apply label encoding\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "\n",
    "# Verify label encoding\n",
    "print(f\"\\nLabel ID range: {df['label_id'].min()} to {df['label_id'].max()}\")\n",
    "print(\"Sample with encoded labels:\")\n",
    "print(df[['text', 'label', 'label_id']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a226e57e-ba60-472e-bcc4-5c7a55b07b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set size: 200\n",
      "Test set size: 50\n",
      "\n",
      "Dataset train features: {'text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n",
      "Dataset test features: {'text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Split data with stratification\n",
    "df_train, df_test = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label']  # Ensure balanced splits\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set size: {len(df_train)}\")\n",
    "print(f\"Test set size: {len(df_test)}\")\n",
    "\n",
    "# Create datasets with only necessary columns\n",
    "train_data = df_train[['text', 'label_id']].copy()\n",
    "test_data = df_test[['text', 'label_id']].copy()\n",
    "\n",
    "# Rename label_id to labels for transformers\n",
    "train_data = train_data.rename(columns={'label_id': 'labels'})\n",
    "test_data = test_data.rename(columns={'label_id': 'labels'})\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "dataset_train = Dataset.from_pandas(train_data)\n",
    "dataset_test = Dataset.from_pandas(test_data)\n",
    "\n",
    "print(f\"\\nDataset train features: {dataset_train.features}\")\n",
    "print(f\"Dataset test features: {dataset_test.features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68934285-93ec-487b-9f0f-c3a4f7bdb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4c42fa-d068-4439-8c72-df3861ee294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████| 200/200 [00:00<00:00, 36226.50 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████| 50/50 [00:00<00:00, 22093.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train dataset features: {'labels': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "Final test dataset features: {'labels': Value(dtype='int64', id=None), '__index_level_0__': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text with proper padding and truncation\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=512,\n",
    "        return_tensors=None  # Don't convert to tensors here\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "dataset_train = dataset_train.map(tokenize_function, batched=True)\n",
    "dataset_test = dataset_test.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns and set format\n",
    "dataset_train = dataset_train.remove_columns(['text'])\n",
    "dataset_test = dataset_test.remove_columns(['text'])\n",
    "\n",
    "print(f\"Final train dataset features: {dataset_train.features}\")\n",
    "print(f\"Final test dataset features: {dataset_test.features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7cc929e-c48f-4c00-9116-5bb7acbddfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model with 2 classes...\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "print(f\"\\nLoading model with {len(label2id)} classes...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55070cf1-c759-4177-8f67-78311c134d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d02ce11-d28a-4365-bed3-eef624cecb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.692682</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.673943</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.514562</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n",
      "Training loss: 0.6660\n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "  eval_loss: 0.5146\n",
      "  eval_accuracy: 0.9400\n",
      "  eval_runtime: 0.2396\n",
      "  eval_samples_per_second: 208.6700\n",
      "  eval_steps_per_second: 29.2140\n",
      "  epoch: 3.0000\n",
      "\n",
      "Saving model...\n",
      "Model saved successfully!\n",
      "\n",
      "Testing predictions:\n",
      "Error during training: Torch not compiled with CUDA enabled\n",
      "Error type: AssertionError\n",
      "\n",
      "Debug information:\n",
      "Train dataset sample: {'labels': 0, '__index_level_0__': 240, 'input_ids': [101, 2318, 2383, 5729, 6402, 2006, 2849, 1998, 2009, 2074, 2318, 102, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Labels type: <class 'int'>\n",
      "Labels value: 0\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,  # Reduced for faster training\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    save_total_limit=2,  # Only keep 2 best models\n",
    "    warmup_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=tokenizer,  # Use processing_class instead of deprecated tokenizer\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "try:\n",
    "    # Train the model\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    print(f\"Evaluation results:\")\n",
    "    for key, value in eval_result.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving model...\")\n",
    "    trainer.save_model(\"./symptom_classifier_model\")\n",
    "    tokenizer.save_pretrained(\"./symptom_classifier_model\")\n",
    "    \n",
    "    # Save label mappings\n",
    "    import json\n",
    "    with open(\"./symptom_classifier_model/label_mappings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"label2id\": label2id,\n",
    "            \"id2label\": id2label\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(\"Model saved successfully!\")\n",
    "    \n",
    "    # Test prediction function\n",
    "    def predict_symptom(text):\n",
    "        \"\"\"Test prediction on new text\"\"\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        # Move inputs to GPU\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "            confidence = predictions.max().item()\n",
    "            \n",
    "        return {\n",
    "            \"predicted_label\": id2label[predicted_class],\n",
    "            \"confidence\": confidence,\n",
    "            \"all_scores\": {id2label[i]: score.item() for i, score in enumerate(predictions[0])}\n",
    "        }\n",
    "    \n",
    "    # Test with sample texts\n",
    "    print(\"\\nTesting predictions:\")\n",
    "    test_texts = [\n",
    "        \"I have a severe headache and feel nauseous\",\n",
    "        \"My chest hurts and I'm having trouble breathing\",\n",
    "        \"I fell and my ankle is swollen and painful\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        result = predict_symptom(text)\n",
    "        print(f\"\\nText: '{text}'\")\n",
    "        print(f\"Predicted: {result['predicted_label']} (confidence: {result['confidence']:.3f})\")\n",
    "        print(\"Top 3 scores:\")\n",
    "        sorted_scores = sorted(result['all_scores'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        for label, score in sorted_scores:\n",
    "            print(f\"  {label}: {score:.3f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Debug information\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Train dataset sample: {dataset_train[0]}\")\n",
    "    print(f\"Labels type: {type(dataset_train[0]['labels'])}\")\n",
    "    print(f\"Labels value: {dataset_train[0]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639b7804-9a21-43a6-bae2-630f5981704b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./1text_model/tokenizer_config.json',\n",
       " './1text_model/special_tokens_map.json',\n",
       " './1text_model/vocab.txt',\n",
       " './1text_model/added_tokens.json',\n",
       " './1text_model/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./1text_model\")\n",
    "tokenizer.save_pretrained(\"./1text_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb801fd-44bc-4104-aade-e0037525cb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c59ee3-c6a7-4b7f-b5c9-f5db7b696c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f9724-d699-4aa9-98bf-7e2f651ab8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc429f-5f14-4a8a-9e54-cf702d69ddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdec77b-af19-4753-9025-1be88814006c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
