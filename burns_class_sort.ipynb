{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb64d2a-e81a-4bde-8894-66ffbd3ce199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16  # Adjust if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e873d77-f9ec-4eb7-8da3-b641a8bba3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('burn_classification_model_final2.h5', compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3d8523-14ff-40b5-9f47-7cb19c406e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocess_image(image_path, target_size=IMG_SIZE):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        aspect_ratio = w / h\n",
    "        if aspect_ratio > 1:\n",
    "            new_w = target_size[0]\n",
    "            new_h = int(target_size[0] / aspect_ratio)\n",
    "        else:\n",
    "            new_h = target_size[1]\n",
    "            new_w = int(target_size[1] * aspect_ratio)\n",
    "        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "        delta_w = target_size[0] - new_w\n",
    "        delta_h = target_size[1] - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "        # CLAHE (optional if you used in training)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return np.zeros((*target_size, 3), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1fc79e-20fc-4dd5-b845-45740db3a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown burn images: 1227\n"
     ]
    }
   ],
   "source": [
    "df_unknown = pd.read_csv(\"burns_unknown_degree.csv\")\n",
    "print(f\"Number of unknown burn images: {len(df_unknown)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08d27ca-83d9-411d-ab55-391ac159c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(df, model, threshold=0.3):\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    for start in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "        end = min(start + BATCH_SIZE, len(df))\n",
    "        batch_files = df['filepath'].iloc[start:end]\n",
    "        batch_imgs = np.stack([advanced_preprocess_image(f) for f in batch_files])\n",
    "        batch_probs = model.predict(batch_imgs, verbose=0)\n",
    "        all_probs.extend(batch_probs.flatten())\n",
    "        batch_preds = (batch_probs > threshold).astype(int).flatten()\n",
    "        all_preds.extend(batch_preds)\n",
    "    return np.array(all_preds), np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2aa8b7d-8052-4455-bab7-55ea7956618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:05<00:00, 12.91it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_labels, pred_probs = predict_images(df_unknown, model, threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc3e80a-ef76-4c09-b7e9-33c3bdc6e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: burns_unknown_labeled.csv\n",
      "predicted_label_str\n",
      "3rd degree        730\n",
      "1st/2nd degree    497\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_unknown['predicted_label'] = pred_labels\n",
    "df_unknown['predicted_prob'] = pred_probs\n",
    "df_unknown['predicted_label_str'] = df_unknown['predicted_label'].map({0: \"1st/2nd degree\", 1: \"3rd degree\"})\n",
    "\n",
    "# Save to CSV for review or merging\n",
    "df_unknown.to_csv(\"burns_unknown_labeled.csv\", index=False)\n",
    "print(\"Saved: burns_unknown_labeled.csv\")\n",
    "print(df_unknown['predicted_label_str'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d25ff8-65b2-4067-bca4-971996630289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Low confidence cases (<0.7):\n",
      "                                             filepath  predicted_prob  \\\n",
      "0   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.030050   \n",
      "1   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.184415   \n",
      "2   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.415353   \n",
      "3   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.018261   \n",
      "4   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.151950   \n",
      "5   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.517774   \n",
      "6   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.266277   \n",
      "7   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.596577   \n",
      "9   C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.674850   \n",
      "11  C:\\Users\\dabne\\.cache\\kagglehub\\datasets\\shubh...        0.055434   \n",
      "\n",
      "   predicted_label_str  \n",
      "0       1st/2nd degree  \n",
      "1       1st/2nd degree  \n",
      "2           3rd degree  \n",
      "3       1st/2nd degree  \n",
      "4       1st/2nd degree  \n",
      "5           3rd degree  \n",
      "6       1st/2nd degree  \n",
      "7           3rd degree  \n",
      "9           3rd degree  \n",
      "11      1st/2nd degree  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLow confidence cases (<0.7):\")\n",
    "print(df_unknown[df_unknown['predicted_prob'] < 0.7][['filepath', 'predicted_prob', 'predicted_label_str']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2670f715-fd35-4ae1-84e4-a0722665a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load new predictions\n",
    "df_unknown = pd.read_csv('burns_unknown_labeled.csv')\n",
    "\n",
    "# Load original labeled datasets\n",
    "df_1and2 = pd.read_csv('burns_1and2.csv')   # original 1st/2nd degree, labeled 0\n",
    "df_3 = pd.read_csv('burns_3rd.csv')         # original 3rd degree, labeled 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d95200ae-4c8b-4210-bd37-bd09ea3edb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 266 of 1227 predictions (confidence >= 0.7)\n",
      "predicted_label_str\n",
      "3rd degree    266\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Only keep high-confidence (>= 0.7)\n",
    "high_conf = df_unknown[df_unknown['predicted_prob'] >= 0.7].copy()\n",
    "\n",
    "print(f\"Keeping {len(high_conf)} of {len(df_unknown)} predictions (confidence >= 0.7)\")\n",
    "print(high_conf['predicted_label_str'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0852dfb2-9d7e-4df8-adb4-14b8ca6e0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_label\n",
      "0    4876\n",
      "1    1289\n",
      "Name: count, dtype: int64\n",
      "Saved: burns_dataset_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Ensure consistent columns\n",
    "df_1and2 = df_1and2.rename(columns={'label': 'binary_label'})\n",
    "df_3 = df_3.rename(columns={'label': 'binary_label'})\n",
    "\n",
    "# 2. Standardize all labels to numeric 0/1\n",
    "df_1and2['binary_label'] = 0  # All 1st/2nd degree\n",
    "df_3['binary_label'] = 1      # All 3rd degree\n",
    "high_conf['binary_label'] = high_conf['predicted_label']  # already numeric\n",
    "\n",
    "# 3. Keep only necessary columns\n",
    "df_1and2 = df_1and2[['filepath', 'binary_label']]\n",
    "df_3 = df_3[['filepath', 'binary_label']]\n",
    "high_conf = high_conf[['filepath', 'binary_label']]\n",
    "\n",
    "# 4. Combine and save\n",
    "df_all = pd.concat([df_1and2, df_3, high_conf], ignore_index=True)\n",
    "df_all.to_csv('burns_dataset_expanded.csv', index=False)\n",
    "print(df_all['binary_label'].value_counts())\n",
    "print(\"Saved: burns_dataset_expanded.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb87f783-3524-4948-86e2-8cfbad6f2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined dataset size: 6165\n",
      "binary_label\n",
      "0    4876\n",
      "1    1289\n",
      "Name: count, dtype: int64\n",
      "Saved: burns_dataset_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine all (original and new labeled)\n",
    "df_all = pd.concat([df_1and2, df_3, high_conf], ignore_index=True)\n",
    "\n",
    "print(f\"Final combined dataset size: {len(df_all)}\")\n",
    "print(df_all['binary_label'].value_counts())\n",
    "\n",
    "# Save\n",
    "df_all.to_csv('burns_dataset_expanded.csv', index=False)\n",
    "print(\"Saved: burns_dataset_expanded.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f2322-0074-4b08-a699-e1a8c242cb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32618bff-734b-40ad-968c-834d448feccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171439aa-b8cc-4857-bbef-8c4403d066ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc5f00-dd79-43e2-869f-a713243c213c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5ed14-4479-4099-96c9-366a77ad5edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b9181-1b8e-4d4e-8f18-195962758fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ce18c-b3be-4cca-82e1-8e3a13925cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
