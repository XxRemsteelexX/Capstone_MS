{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403fc71a-e671-42e4-8dad-c361d9499a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical Dataset Rebuilder\n",
      "========================================\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Medical Dataset Rebuilder\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1b98cd-76ac-4bbe-a2bb-6da74f51211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define MedicalDatasetBuilder Class - Part 1\n",
    "class MedicalDatasetBuilder:\n",
    "    \"\"\"\n",
    "    Build medical image dataset from existing CSV files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_directory=\".\"):\n",
    "        \"\"\"\n",
    "        Initialize with directory containing CSV files\n",
    "        \n",
    "        Args:\n",
    "            csv_directory (str): Directory containing the CSV files\n",
    "        \"\"\"\n",
    "        self.csv_directory = Path(csv_directory)\n",
    "        self.master_df = None\n",
    "        \n",
    "    def load_existing_csvs(self):\n",
    "        \"\"\"\n",
    "        Load all existing CSV files and identify available datasets\n",
    "        \"\"\"\n",
    "        print(\"Scanning for existing CSV files...\")\n",
    "        \n",
    "        # List of expected CSV files based on your original notebook\n",
    "        expected_csvs = [\n",
    "            \"burns_1and2.csv\",\n",
    "            \"burns_3rd.csv\", \n",
    "            \"burns_unknown_degree.csv\",\n",
    "            \"wounds.csv\",\n",
    "            \"urgent_care_images_master.csv\",\n",
    "            \"urgent_care_images_master_no_wound_burns.csv\",\n",
    "            \"urgent_care_images_master_plus_yasin.csv\",\n",
    "            \"urgent_care_images_master_final.csv\"\n",
    "        ]\n",
    "        \n",
    "        found_csvs = []\n",
    "        missing_csvs = []\n",
    "        \n",
    "        for csv_file in expected_csvs:\n",
    "            csv_path = self.csv_directory / csv_file\n",
    "            if csv_path.exists():\n",
    "                found_csvs.append(csv_file)\n",
    "                size = csv_path.stat().st_size / 1024  # KB\n",
    "                print(f\"  Found: {csv_file} ({size:.1f} KB)\")\n",
    "            else:\n",
    "                missing_csvs.append(csv_file)\n",
    "                print(f\"  Missing: {csv_file}\")\n",
    "        \n",
    "        if missing_csvs:\n",
    "            print(f\"\\nMissing {len(missing_csvs)} CSV files\")\n",
    "        \n",
    "        return found_csvs, missing_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559742b1-d8dd-4ee7-af18-67b44cb0f37c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3152934310.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef build_from_components(self):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Add Component Building Methods\n",
    "    def build_from_components(self):\n",
    "        \"\"\"\n",
    "        Build master dataset from individual component CSV files\n",
    "        \"\"\"\n",
    "        print(\"\\nBuilding dataset from component CSV files...\")\n",
    "        \n",
    "        dataframes = []\n",
    "        \n",
    "        # Try to load burn datasets\n",
    "        burn_files = [\"burns_1and2.csv\", \"burns_3rd.csv\", \"burns_unknown_degree.csv\"]\n",
    "        for burn_file in burn_files:\n",
    "            csv_path = self.csv_directory / burn_file\n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                print(f\"  Loaded {burn_file}: {len(df)} images\")\n",
    "                dataframes.append(df)\n",
    "            else:\n",
    "                print(f\"  Skipping missing {burn_file}\")\n",
    "        \n",
    "        # Try to load wound dataset\n",
    "        wounds_path = self.csv_directory / \"wounds.csv\"\n",
    "        if wounds_path.exists():\n",
    "            df_wounds = pd.read_csv(wounds_path)\n",
    "            print(f\"  Loaded wounds.csv: {len(df_wounds)} images\")\n",
    "            dataframes.append(df_wounds)\n",
    "        else:\n",
    "            print(f\"  Skipping missing wounds.csv\")\n",
    "        \n",
    "        if dataframes:\n",
    "            # Combine all dataframes\n",
    "            self.master_df = pd.concat(dataframes, ignore_index=True)\n",
    "            print(f\"\\nCombined dataset: {len(self.master_df)} total images\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"No component CSV files found\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a921206-4fc1-4a6b-b7da-a1c789e6fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Add Master File Loading Method\n",
    "    def load_existing_master(self):\n",
    "        \"\"\"\n",
    "        Load existing master CSV file if available\n",
    "        \"\"\"\n",
    "        print(\"\\nLooking for existing master datasets...\")\n",
    "        \n",
    "        # Priority order for master files (most processed first)\n",
    "        master_files = [\n",
    "            \"urgent_care_images_master_final.csv\",\n",
    "            \"urgent_care_images_master_plus_yasin.csv\", \n",
    "            \"urgent_care_images_master_no_wound_burns.csv\",\n",
    "            \"urgent_care_images_master.csv\"\n",
    "        ]\n",
    "        \n",
    "        for master_file in master_files:\n",
    "            csv_path = self.csv_directory / master_file\n",
    "            if csv_path.exists():\n",
    "                self.master_df = pd.read_csv(csv_path)\n",
    "                print(f\"  Loaded {master_file}: {len(self.master_df)} images\")\n",
    "                return True\n",
    "        \n",
    "        print(\"  No master CSV files found\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56845461-a27a-4a4d-861e-d262739352da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Add File Path and Validation Methods\n",
    "    def fix_file_paths(self, new_base_path=None):\n",
    "        \"\"\"\n",
    "        Fix file paths in the dataset to point to correct locations\n",
    "        \n",
    "        Args:\n",
    "            new_base_path (str): New base path for images, if different from CSV\n",
    "        \"\"\"\n",
    "        if self.master_df is None:\n",
    "            print(\"No dataset loaded\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nAnalyzing file paths...\")\n",
    "        \n",
    "        # Show sample of current paths\n",
    "        print(\"Sample current file paths:\")\n",
    "        for i, path in enumerate(self.master_df['filepath'].head(3)):\n",
    "            print(f\"  {i+1}. {path}\")\n",
    "        \n",
    "        if new_base_path:\n",
    "            print(f\"\\nUpdating base path to: {new_base_path}\")\n",
    "            \n",
    "            def update_path(old_path):\n",
    "                # Extract just the filename from old path\n",
    "                filename = Path(old_path).name\n",
    "                return str(Path(new_base_path) / filename)\n",
    "            \n",
    "            self.master_df['filepath'] = self.master_df['filepath'].apply(update_path)\n",
    "            print(\"File paths updated\")\n",
    "    \n",
    "    def validate_files(self, sample_size=10):\n",
    "        \"\"\"\n",
    "        Validate that files exist at specified paths\n",
    "        \n",
    "        Args:\n",
    "            sample_size (int): Number of files to check\n",
    "        \"\"\"\n",
    "        if self.master_df is None:\n",
    "            print(\"No dataset loaded\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nValidating file existence (checking {sample_size} samples)...\")\n",
    "        \n",
    "        sample_df = self.master_df.sample(min(sample_size, len(self.master_df)))\n",
    "        \n",
    "        existing_count = 0\n",
    "        missing_count = 0\n",
    "        \n",
    "        for _, row in sample_df.iterrows():\n",
    "            if Path(row['filepath']).exists():\n",
    "                existing_count += 1\n",
    "            else:\n",
    "                missing_count += 1\n",
    "                print(f\"  Missing: {row['filepath']}\")\n",
    "        \n",
    "        print(f\"\\nValidation Results:\")\n",
    "        print(f\"  Found: {existing_count}/{sample_size} files\")\n",
    "        print(f\"  Missing: {missing_count}/{sample_size} files\")\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(\"\\nSuggestion: Update file paths using fix_file_paths() method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27dbfb9-8822-41b5-9db1-7ec207a3f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Add Data Cleaning Methods\n",
    "    def clean_dataset(self, remove_classes=None, merge_classes=None):\n",
    "        \"\"\"\n",
    "        Clean and standardize the dataset\n",
    "        \n",
    "        Args:\n",
    "            remove_classes (list): List of class labels to remove\n",
    "            merge_classes (dict): Dict mapping old labels to new labels\n",
    "        \"\"\"\n",
    "        if self.master_df is None:\n",
    "            print(\"No dataset loaded\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nCleaning dataset...\")\n",
    "        \n",
    "        original_size = len(self.master_df)\n",
    "        \n",
    "        # Remove unwanted classes\n",
    "        if remove_classes:\n",
    "            print(f\"  Removing classes: {remove_classes}\")\n",
    "            self.master_df = self.master_df[~self.master_df['label'].isin(remove_classes)]\n",
    "            print(f\"    Removed {original_size - len(self.master_df)} images\")\n",
    "        \n",
    "        # Merge/standardize classes\n",
    "        if merge_classes:\n",
    "            print(f\"  Merging classes: {merge_classes}\")\n",
    "            self.master_df['label'] = self.master_df['label'].replace(merge_classes)\n",
    "        \n",
    "        # Standard cleaning from original notebook\n",
    "        standard_merges = {\n",
    "            \"wound_laseration\": \"wound_laceration\",  # Fix typo\n",
    "            \"wound_stab_wound\": \"wound_laceration\",  # Merge similar classes\n",
    "        }\n",
    "        \n",
    "        self.master_df['label'] = self.master_df['label'].replace(standard_merges)\n",
    "        \n",
    "        print(f\"Cleaned dataset: {len(self.master_df)} images remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c06ef4-6829-4078-b817-ef5ebb5c23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Add Information Display and Save Methods\n",
    "    def show_dataset_info(self):\n",
    "        \"\"\"\n",
    "        Display comprehensive dataset information\n",
    "        \"\"\"\n",
    "        if self.master_df is None:\n",
    "            print(\"No dataset loaded\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nDataset Information\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        print(f\"Total Images: {len(self.master_df):,}\")\n",
    "        print(f\"Total Classes: {self.master_df['label'].nunique()}\")\n",
    "        \n",
    "        print(\"\\nClass Distribution:\")\n",
    "        class_counts = self.master_df['label'].value_counts()\n",
    "        for label, count in class_counts.items():\n",
    "            percentage = (count / len(self.master_df)) * 100\n",
    "            print(f\"  {label:25} {count:>6,} images ({percentage:5.1f}%)\")\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        class_counts.plot(kind='bar', color='steelblue')\n",
    "        plt.title('Medical Image Dataset - Class Distribution')\n",
    "        plt.xlabel('Class Label')\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return class_counts\n",
    "    \n",
    "    def save_dataset(self, filename=\"medical_dataset_fixed.csv\"):\n",
    "        \"\"\"\n",
    "        Save the cleaned dataset\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Output filename\n",
    "        \"\"\"\n",
    "        if self.master_df is None:\n",
    "            print(\"No dataset loaded\")\n",
    "            return\n",
    "        \n",
    "        output_path = self.csv_directory / filename\n",
    "        self.master_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved dataset to: {output_path}\")\n",
    "        print(f\"   Total images: {len(self.master_df):,}\")\n",
    "        print(f\"   Total classes: {self.master_df['label'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd44b7-9f75-493c-84ad-e6f661b41428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Initialize and Scan for Files\n",
    "# Initialize builder\n",
    "builder = MedicalDatasetBuilder()\n",
    "\n",
    "# Scan for existing files\n",
    "found_csvs, missing_csvs = builder.load_existing_csvs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350e09c-83ad-46ea-a38d-290b92bca8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Load Dataset\n",
    "# Try to load existing master file first\n",
    "if not builder.load_existing_master():\n",
    "    # If no master file, build from components\n",
    "    if not builder.build_from_components():\n",
    "        print(\"Failed to load any dataset files\")\n",
    "    else:\n",
    "        print(\"Successfully built dataset from components\")\n",
    "else:\n",
    "    print(\"Successfully loaded existing master dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff445d-9789-4fdb-9c1d-33e0ed7c0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Display Current Dataset Info\n",
    "if builder.master_df is not None:\n",
    "    builder.show_dataset_info()\n",
    "else:\n",
    "    print(\"No dataset available to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272b453-82bb-4f1e-b1f0-126a56fb7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Optional - Fix File Paths (uncomment and modify if needed)\n",
    "# If your images are in a different location, uncomment and modify this:\n",
    "# new_image_path = \"/path/to/your/images\"\n",
    "# builder.fix_file_paths(new_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cb470-cd5d-4b49-a6fe-845edcd3bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Optional - Validate File Existence (uncomment to check)\n",
    "# Uncomment to check if files exist at current paths:\n",
    "# builder.validate_files(sample_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35588108-d504-48e0-ac64-fdbf2623e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Clean Dataset\n",
    "# Remove problematic classes\n",
    "remove_classes = [\n",
    "    \"wound_burns\",  # Duplicate with burn classes\n",
    "    \"wound_ingrown_nails\",  # Not relevant for urgent care\n",
    "    \"wound_normal\",  # Not a medical condition\n",
    "    \"wound_surgical_wounds\"  # Too specific/clinical\n",
    "]\n",
    "\n",
    "if builder.master_df is not None:\n",
    "    builder.clean_dataset(remove_classes=remove_classes)\n",
    "else:\n",
    "    print(\"No dataset loaded for cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e3d0d-1f8f-497a-bfd8-3ba9f517ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Show Final Dataset Info\n",
    "if builder.master_df is not None:\n",
    "    print(\"\\nFinal Dataset:\")\n",
    "    final_counts = builder.show_dataset_info()\n",
    "else:\n",
    "    print(\"No dataset available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7cc502-5b2d-404e-837f-673cba6a4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell 15: Save Rebuilt Dataset with Original Names\n",
    "if builder.master_df is not None:\n",
    "    # Save with the original expected filename that other notebooks use\n",
    "    builder.save_dataset(\"urgent_care_images_master_final.csv\")\n",
    "    \n",
    "    # Also save backup versions for safety\n",
    "    builder.save_dataset(\"urgent_care_images_master.csv\")\n",
    "    builder.save_dataset(\"urgent_care_images_master_no_wound_burns.csv\")\n",
    "    \n",
    "    print(\"\\nDataset rebuilding complete!\")\n",
    "    print(\"Saved with original filenames that other notebooks expect:\")\n",
    "    print(\"- urgent_care_images_master_final.csv (main file)\")\n",
    "    print(\"- urgent_care_images_master.csv (backup)\")\n",
    "    print(\"- urgent_care_images_master_no_wound_burns.csv (backup)\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Verify file paths point to correct image locations\")\n",
    "    print(\"2. Update paths using builder.fix_file_paths() if needed\")\n",
    "    print(\"3. Run builder.validate_files() to check file existence\")\n",
    "else:\n",
    "    print(\"No dataset to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe7019-bd48-45d6-a4d4-67f2e2127276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 16: Optional - Quick Dataset Exploration\n",
    "# Uncomment to explore the dataset further:\n",
    "# if builder.master_df is not None:\n",
    "#     print(\"\\nFirst few rows of the dataset:\")\n",
    "#     print(builder.master_df.head())\n",
    "#     \n",
    "#     print(\"\\nDataset columns:\")\n",
    "#     print(builder.master_df.columns.tolist())\n",
    "#     \n",
    "#     print(\"\\nDataset shape:\")\n",
    "#     print(builder.master_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95c6b0-c699-482c-8e45-3cdaf4886400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
