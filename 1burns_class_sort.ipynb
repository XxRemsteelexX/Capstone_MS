{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb64d2a-e81a-4bde-8894-66ffbd3ce199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 15:04:20.796083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-25 15:04:20.803720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753473860.812176   42317 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753473860.814665   42317 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753473860.821259   42317 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753473860.821266   42317 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753473860.821267   42317 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753473860.821267   42317 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-25 15:04:20.823494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Using MirroredStrategy with 2 GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753473862.270916   42317 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21024 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:02:00.0, compute capability: 8.6\n",
      "I0000 00:00:1753473862.272300   42317 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21255 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers, models, callbacks, Model\n",
    "from tensorflow.keras.applications import EfficientNetV2B0, DenseNet121\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Detect and list GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs detected: {gpus}\")\n",
    "\n",
    "# Optionally, set memory growth to avoid OOM errors\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "#  use all GPUs for inference (not just the first one)\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} GPUs\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16  # Adjust if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e873d77-f9ec-4eb7-8da3-b641a8bba3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('burn_triage_cnn.h5', compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3d8523-14ff-40b5-9f47-7cb19c406e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_preprocess_image(image_path, target_size=IMG_SIZE):\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        aspect_ratio = w / h\n",
    "        if aspect_ratio > 1:\n",
    "            new_w = target_size[0]\n",
    "            new_h = int(target_size[0] / aspect_ratio)\n",
    "        else:\n",
    "            new_h = target_size[1]\n",
    "            new_w = int(target_size[1] * aspect_ratio)\n",
    "        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "        delta_w = target_size[0] - new_w\n",
    "        delta_h = target_size[1] - new_h\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "        img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "        # CLAHE (optional if you used in training)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return np.zeros((*target_size, 3), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1fc79e-20fc-4dd5-b845-45740db3a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown burn images: 1227\n"
     ]
    }
   ],
   "source": [
    "df_unknown = pd.read_csv(\"burns_unknown_degree.csv\")\n",
    "print(f\"Number of unknown burn images: {len(df_unknown)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a08d27ca-83d9-411d-ab55-391ac159c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(df, model, threshold=0.3):\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    for start in tqdm(range(0, len(df), BATCH_SIZE)):\n",
    "        end = min(start + BATCH_SIZE, len(df))\n",
    "        batch_files = df['filepath'].iloc[start:end]\n",
    "        batch_imgs = np.stack([advanced_preprocess_image(f) for f in batch_files])\n",
    "        batch_probs = model.predict(batch_imgs, verbose=0)\n",
    "        all_probs.extend(batch_probs.flatten())\n",
    "        batch_preds = (batch_probs > threshold).astype(int).flatten()\n",
    "        all_preds.extend(batch_preds)\n",
    "    return np.array(all_preds), np.array(all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2aa8b7d-8052-4455-bab7-55ea7956618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                 | 0/77 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753473862.897456   42445 service.cc:152] XLA service 0x7a23640067f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753473862.897475   42445 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "I0000 00:00:1753473862.897477   42445 service.cc:160]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-07-25 15:04:22.901503: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753473862.920437   42445 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1753473863.273512   42445 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "100%|████████████████████████████████████████████████████████| 77/77 [00:06<00:00, 11.77it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_labels, pred_probs = predict_images(df_unknown, model, threshold=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc3e80a-ef76-4c09-b7e9-33c3bdc6e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: burns_unknown_labeled.csv\n",
      "predicted_label_str\n",
      "3rd degree        1160\n",
      "1st/2nd degree      67\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_unknown['predicted_label'] = pred_labels\n",
    "df_unknown['predicted_prob'] = pred_probs\n",
    "df_unknown['predicted_label_str'] = df_unknown['predicted_label'].map({0: \"1st/2nd degree\", 1: \"3rd degree\"})\n",
    "\n",
    "# Save to CSV for review or merging\n",
    "df_unknown.to_csv(\"burns_unknown_labeled.csv\", index=False)\n",
    "print(\"Saved: burns_unknown_labeled.csv\")\n",
    "print(df_unknown['predicted_label_str'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d25ff8-65b2-4067-bca4-971996630289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Low confidence cases (<0.7):\n",
      "                    filepath  predicted_prob predicted_label_str\n",
      "2   burn_dataset/img1400.jpg        0.498493          3rd degree\n",
      "3    burn_dataset/img587.jpg        0.676570          3rd degree\n",
      "4    burn_dataset/img614.jpg        0.581842          3rd degree\n",
      "11   burn_dataset/img508.jpg        0.687151          3rd degree\n",
      "14   burn_dataset/img512.jpg        0.230303      1st/2nd degree\n",
      "19   burn_dataset/img619.jpg        0.566925          3rd degree\n",
      "21  burn_dataset/img1183.jpg        0.554728          3rd degree\n",
      "24   burn_dataset/img671.jpg        0.321389          3rd degree\n",
      "26   burn_dataset/img693.jpg        0.477440          3rd degree\n",
      "30  burn_dataset/img1228.jpg        0.537761          3rd degree\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLow confidence cases (<0.7):\")\n",
    "print(df_unknown[df_unknown['predicted_prob'] < 0.7][['filepath', 'predicted_prob', 'predicted_label_str']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2670f715-fd35-4ae1-84e4-a0722665a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load new predictions\n",
    "df_unknown = pd.read_csv('burns_unknown_labeled.csv')\n",
    "\n",
    "# Load original labeled datasets\n",
    "df_1and2 = pd.read_csv('burns_1and2.csv')   # original 1st/2nd degree, labeled 0\n",
    "df_3 = pd.read_csv('burns_3rd.csv')         # original 3rd degree, labeled 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d95200ae-4c8b-4210-bd37-bd09ea3edb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 752 of 1227 predictions (confidence >= 0.7)\n",
      "predicted_label_str\n",
      "3rd degree    752\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Only keep high-confidence (>= 0.7)\n",
    "high_conf = df_unknown[df_unknown['predicted_prob'] >= 0.7].copy()\n",
    "\n",
    "print(f\"Keeping {len(high_conf)} of {len(df_unknown)} predictions (confidence >= 0.7)\")\n",
    "print(high_conf['predicted_label_str'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0852dfb2-9d7e-4df8-adb4-14b8ca6e0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure consistent columns\n",
    "df_1and2 = df_1and2.rename(columns={'label': 'binary_label'})\n",
    "df_3 = df_3.rename(columns={'label': 'binary_label'})\n",
    "\n",
    "# Standardize all labels to numeric 0/1\n",
    "df_1and2['binary_label'] = 0  # All 1st/2nd degree\n",
    "df_3['binary_label'] = 1      # All 3rd degree\n",
    "high_conf['binary_label'] = high_conf['predicted_label']  # already numeric\n",
    "\n",
    "# Keep only necessary columns\n",
    "df_1and2 = df_1and2[['filepath', 'binary_label']]\n",
    "df_3 = df_3[['filepath', 'binary_label']]\n",
    "high_conf = high_conf[['filepath', 'binary_label']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb87f783-3524-4948-86e2-8cfbad6f2c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined dataset size: 6651\n",
      "binary_label\n",
      "0    4876\n",
      "1    1775\n",
      "Name: count, dtype: int64\n",
      "Saved: burns_dataset_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine all (original and new labeled)\n",
    "df_all = pd.concat([df_1and2, df_3, high_conf], ignore_index=True)\n",
    "\n",
    "print(f\"Final combined dataset size: {len(df_all)}\")\n",
    "print(df_all['binary_label'].value_counts())\n",
    "\n",
    "# Save\n",
    "df_all.to_csv('burns_dataset_expanded.csv', index=False)\n",
    "print(\"Saved: burns_dataset_expanded.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f2322-0074-4b08-a699-e1a8c242cb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32618bff-734b-40ad-968c-834d448feccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171439aa-b8cc-4857-bbef-8c4403d066ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc5f00-dd79-43e2-869f-a713243c213c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5ed14-4479-4099-96c9-366a77ad5edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b9181-1b8e-4d4e-8f18-195962758fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ce18c-b3be-4cca-82e1-8e3a13925cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
